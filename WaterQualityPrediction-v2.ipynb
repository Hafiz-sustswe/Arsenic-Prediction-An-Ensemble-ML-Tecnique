{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, BayesianRidge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib  # Correct import for joblib\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the final feature engineered data\n",
    "df_final = pd.read_csv('final_feature_engineered_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day</th>\n",
       "      <th>Month</th>\n",
       "      <th>Year</th>\n",
       "      <th>Conc. Of  pH</th>\n",
       "      <th>EC</th>\n",
       "      <th>Bicarb</th>\n",
       "      <th>Chlor</th>\n",
       "      <th>Iron</th>\n",
       "      <th>Manganese</th>\n",
       "      <th>Sodium</th>\n",
       "      <th>Potassium</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Log_Arsenic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.213766</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>0.181918</td>\n",
       "      <td>0.529605</td>\n",
       "      <td>0.937836</td>\n",
       "      <td>-0.897475</td>\n",
       "      <td>-0.332173</td>\n",
       "      <td>-0.199134</td>\n",
       "      <td>-0.990882</td>\n",
       "      <td>-0.718107</td>\n",
       "      <td>0.459290</td>\n",
       "      <td>2.712108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.198689</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>0.189731</td>\n",
       "      <td>0.521134</td>\n",
       "      <td>0.922757</td>\n",
       "      <td>-0.903200</td>\n",
       "      <td>-0.345305</td>\n",
       "      <td>-0.198272</td>\n",
       "      <td>-0.970444</td>\n",
       "      <td>-0.688951</td>\n",
       "      <td>0.477924</td>\n",
       "      <td>2.704008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.194115</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>0.203582</td>\n",
       "      <td>0.512210</td>\n",
       "      <td>0.953746</td>\n",
       "      <td>-0.860840</td>\n",
       "      <td>-0.374221</td>\n",
       "      <td>-0.185931</td>\n",
       "      <td>-0.971685</td>\n",
       "      <td>-0.696098</td>\n",
       "      <td>0.463224</td>\n",
       "      <td>2.671262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.204799</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>0.191872</td>\n",
       "      <td>0.549472</td>\n",
       "      <td>0.960658</td>\n",
       "      <td>-0.921168</td>\n",
       "      <td>-0.368752</td>\n",
       "      <td>-0.218919</td>\n",
       "      <td>-0.977194</td>\n",
       "      <td>-0.670931</td>\n",
       "      <td>0.455944</td>\n",
       "      <td>2.705551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.389907</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>-0.409650</td>\n",
       "      <td>-0.473947</td>\n",
       "      <td>-0.564241</td>\n",
       "      <td>-1.396437</td>\n",
       "      <td>-0.353426</td>\n",
       "      <td>-0.788518</td>\n",
       "      <td>-1.319128</td>\n",
       "      <td>0.775543</td>\n",
       "      <td>-0.114794</td>\n",
       "      <td>2.811246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.389169</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>-0.326212</td>\n",
       "      <td>-0.507226</td>\n",
       "      <td>-0.535738</td>\n",
       "      <td>-1.405846</td>\n",
       "      <td>-0.380960</td>\n",
       "      <td>-0.752810</td>\n",
       "      <td>-1.323950</td>\n",
       "      <td>0.745343</td>\n",
       "      <td>-0.129277</td>\n",
       "      <td>2.864956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.411476</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>-0.329115</td>\n",
       "      <td>-0.501720</td>\n",
       "      <td>-0.586776</td>\n",
       "      <td>-1.402391</td>\n",
       "      <td>-0.333524</td>\n",
       "      <td>-0.767888</td>\n",
       "      <td>-1.328753</td>\n",
       "      <td>0.806713</td>\n",
       "      <td>-0.082678</td>\n",
       "      <td>2.827142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.402605</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>-0.344410</td>\n",
       "      <td>-0.471387</td>\n",
       "      <td>-0.567157</td>\n",
       "      <td>-1.447269</td>\n",
       "      <td>-0.383242</td>\n",
       "      <td>-0.785012</td>\n",
       "      <td>-1.349649</td>\n",
       "      <td>0.768426</td>\n",
       "      <td>-0.097811</td>\n",
       "      <td>2.847875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.403965</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>-0.961438</td>\n",
       "      <td>-0.985865</td>\n",
       "      <td>-0.578515</td>\n",
       "      <td>0.501863</td>\n",
       "      <td>0.153439</td>\n",
       "      <td>-0.944401</td>\n",
       "      <td>0.111024</td>\n",
       "      <td>-1.265145</td>\n",
       "      <td>0.378072</td>\n",
       "      <td>3.250715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.443295</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>-1.030846</td>\n",
       "      <td>-0.980619</td>\n",
       "      <td>-0.570541</td>\n",
       "      <td>0.514483</td>\n",
       "      <td>0.135911</td>\n",
       "      <td>-0.975615</td>\n",
       "      <td>0.153014</td>\n",
       "      <td>-1.301238</td>\n",
       "      <td>0.385498</td>\n",
       "      <td>3.235086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.348919</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>-0.987501</td>\n",
       "      <td>-1.009261</td>\n",
       "      <td>-0.564257</td>\n",
       "      <td>0.500792</td>\n",
       "      <td>0.116623</td>\n",
       "      <td>-0.960404</td>\n",
       "      <td>0.108902</td>\n",
       "      <td>-1.306280</td>\n",
       "      <td>0.404702</td>\n",
       "      <td>3.283278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.413506</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>-0.980599</td>\n",
       "      <td>-0.977828</td>\n",
       "      <td>-0.536462</td>\n",
       "      <td>0.554115</td>\n",
       "      <td>0.160662</td>\n",
       "      <td>-0.945686</td>\n",
       "      <td>0.142984</td>\n",
       "      <td>-1.325308</td>\n",
       "      <td>0.406172</td>\n",
       "      <td>3.241119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1.191048</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>0.742132</td>\n",
       "      <td>0.722244</td>\n",
       "      <td>-0.551269</td>\n",
       "      <td>0.298711</td>\n",
       "      <td>-0.231366</td>\n",
       "      <td>0.086939</td>\n",
       "      <td>1.385282</td>\n",
       "      <td>-0.790186</td>\n",
       "      <td>0.511498</td>\n",
       "      <td>3.298366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1.163172</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>0.765892</td>\n",
       "      <td>0.766761</td>\n",
       "      <td>-0.572920</td>\n",
       "      <td>0.248559</td>\n",
       "      <td>-0.290601</td>\n",
       "      <td>0.124875</td>\n",
       "      <td>1.418864</td>\n",
       "      <td>-0.782977</td>\n",
       "      <td>0.490790</td>\n",
       "      <td>3.230656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1.148418</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>0.714796</td>\n",
       "      <td>0.699271</td>\n",
       "      <td>-0.543553</td>\n",
       "      <td>0.316503</td>\n",
       "      <td>-0.267457</td>\n",
       "      <td>0.089889</td>\n",
       "      <td>1.387848</td>\n",
       "      <td>-0.831066</td>\n",
       "      <td>0.518893</td>\n",
       "      <td>3.259957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-1.139837</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>0.772794</td>\n",
       "      <td>0.745214</td>\n",
       "      <td>-0.602549</td>\n",
       "      <td>0.321087</td>\n",
       "      <td>-0.225028</td>\n",
       "      <td>0.104677</td>\n",
       "      <td>1.387150</td>\n",
       "      <td>-0.807854</td>\n",
       "      <td>0.515434</td>\n",
       "      <td>3.249262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.216617</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>1.442548</td>\n",
       "      <td>-1.181108</td>\n",
       "      <td>-0.547141</td>\n",
       "      <td>-0.492377</td>\n",
       "      <td>-0.176832</td>\n",
       "      <td>0.877526</td>\n",
       "      <td>-0.718272</td>\n",
       "      <td>-0.254101</td>\n",
       "      <td>-0.339829</td>\n",
       "      <td>4.245306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.224547</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>1.524893</td>\n",
       "      <td>-1.220027</td>\n",
       "      <td>-0.610562</td>\n",
       "      <td>-0.460338</td>\n",
       "      <td>-0.153983</td>\n",
       "      <td>0.833477</td>\n",
       "      <td>-0.699742</td>\n",
       "      <td>-0.253327</td>\n",
       "      <td>-0.309917</td>\n",
       "      <td>4.265209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.204835</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>1.480022</td>\n",
       "      <td>-1.204236</td>\n",
       "      <td>-0.546412</td>\n",
       "      <td>-0.415926</td>\n",
       "      <td>-0.189666</td>\n",
       "      <td>0.831714</td>\n",
       "      <td>-0.710541</td>\n",
       "      <td>-0.255480</td>\n",
       "      <td>-0.349488</td>\n",
       "      <td>4.247390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.176050</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>1.443563</td>\n",
       "      <td>-1.194565</td>\n",
       "      <td>-0.568110</td>\n",
       "      <td>-0.500286</td>\n",
       "      <td>-0.205625</td>\n",
       "      <td>0.816156</td>\n",
       "      <td>-0.731213</td>\n",
       "      <td>-0.279034</td>\n",
       "      <td>-0.346970</td>\n",
       "      <td>4.236032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.176482</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>1.484093</td>\n",
       "      <td>-1.150029</td>\n",
       "      <td>-0.490397</td>\n",
       "      <td>-0.412671</td>\n",
       "      <td>-0.211854</td>\n",
       "      <td>0.847092</td>\n",
       "      <td>-0.735555</td>\n",
       "      <td>-0.271758</td>\n",
       "      <td>-0.345410</td>\n",
       "      <td>4.255993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-1.648966</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>-2.492794</td>\n",
       "      <td>1.620694</td>\n",
       "      <td>0.968468</td>\n",
       "      <td>2.213489</td>\n",
       "      <td>1.216120</td>\n",
       "      <td>0.605988</td>\n",
       "      <td>0.736430</td>\n",
       "      <td>1.313136</td>\n",
       "      <td>1.214296</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-1.648966</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>-0.910556</td>\n",
       "      <td>0.061813</td>\n",
       "      <td>0.968468</td>\n",
       "      <td>1.875330</td>\n",
       "      <td>2.533678</td>\n",
       "      <td>1.498620</td>\n",
       "      <td>0.865038</td>\n",
       "      <td>-0.056871</td>\n",
       "      <td>1.368144</td>\n",
       "      <td>1.945910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-1.648966</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>-1.474684</td>\n",
       "      <td>1.511905</td>\n",
       "      <td>0.968468</td>\n",
       "      <td>0.163654</td>\n",
       "      <td>1.911760</td>\n",
       "      <td>3.888941</td>\n",
       "      <td>-0.097930</td>\n",
       "      <td>-0.210933</td>\n",
       "      <td>0.953491</td>\n",
       "      <td>2.079442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-1.156934</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>-1.727366</td>\n",
       "      <td>2.376656</td>\n",
       "      <td>2.316229</td>\n",
       "      <td>-1.004502</td>\n",
       "      <td>-1.149010</td>\n",
       "      <td>1.386235</td>\n",
       "      <td>-0.648833</td>\n",
       "      <td>1.196789</td>\n",
       "      <td>1.608201</td>\n",
       "      <td>1.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-1.156934</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>-0.972938</td>\n",
       "      <td>-0.380401</td>\n",
       "      <td>0.968468</td>\n",
       "      <td>0.628290</td>\n",
       "      <td>2.370488</td>\n",
       "      <td>-0.274624</td>\n",
       "      <td>1.795109</td>\n",
       "      <td>0.376643</td>\n",
       "      <td>1.007188</td>\n",
       "      <td>1.609438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-1.156934</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>0.321770</td>\n",
       "      <td>0.315978</td>\n",
       "      <td>2.316229</td>\n",
       "      <td>0.966982</td>\n",
       "      <td>-1.301602</td>\n",
       "      <td>-1.178592</td>\n",
       "      <td>1.986566</td>\n",
       "      <td>-0.483373</td>\n",
       "      <td>1.375757</td>\n",
       "      <td>1.945910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-1.156934</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>0.806712</td>\n",
       "      <td>0.284894</td>\n",
       "      <td>0.968468</td>\n",
       "      <td>-0.131904</td>\n",
       "      <td>-1.099014</td>\n",
       "      <td>-0.431062</td>\n",
       "      <td>2.115548</td>\n",
       "      <td>2.055641</td>\n",
       "      <td>1.411283</td>\n",
       "      <td>1.609438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-1.156934</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>0.746339</td>\n",
       "      <td>0.703670</td>\n",
       "      <td>-0.558956</td>\n",
       "      <td>0.300733</td>\n",
       "      <td>-0.265015</td>\n",
       "      <td>0.112192</td>\n",
       "      <td>1.397733</td>\n",
       "      <td>-0.814273</td>\n",
       "      <td>0.510234</td>\n",
       "      <td>3.258097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.685845</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>-0.106261</td>\n",
       "      <td>1.169755</td>\n",
       "      <td>-0.558956</td>\n",
       "      <td>0.361073</td>\n",
       "      <td>-1.405577</td>\n",
       "      <td>-0.152641</td>\n",
       "      <td>-0.172895</td>\n",
       "      <td>1.615090</td>\n",
       "      <td>-2.015707</td>\n",
       "      <td>1.609438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-0.685845</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>-0.290777</td>\n",
       "      <td>1.090550</td>\n",
       "      <td>-0.558956</td>\n",
       "      <td>0.945695</td>\n",
       "      <td>-0.094729</td>\n",
       "      <td>-1.919036</td>\n",
       "      <td>0.262016</td>\n",
       "      <td>-1.255074</td>\n",
       "      <td>-2.109886</td>\n",
       "      <td>1.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-0.685845</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>0.199831</td>\n",
       "      <td>1.375127</td>\n",
       "      <td>0.968468</td>\n",
       "      <td>2.528674</td>\n",
       "      <td>-0.621152</td>\n",
       "      <td>-0.964301</td>\n",
       "      <td>1.150713</td>\n",
       "      <td>1.669452</td>\n",
       "      <td>-2.164018</td>\n",
       "      <td>1.945910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.200140</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>1.466277</td>\n",
       "      <td>-0.710772</td>\n",
       "      <td>-2.321421</td>\n",
       "      <td>1.315407</td>\n",
       "      <td>3.496431</td>\n",
       "      <td>0.054447</td>\n",
       "      <td>0.256990</td>\n",
       "      <td>1.313136</td>\n",
       "      <td>-2.417841</td>\n",
       "      <td>1.791759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.200140</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>-0.414149</td>\n",
       "      <td>-0.983449</td>\n",
       "      <td>0.968468</td>\n",
       "      <td>1.281093</td>\n",
       "      <td>-1.301602</td>\n",
       "      <td>1.542931</td>\n",
       "      <td>0.503957</td>\n",
       "      <td>-0.056871</td>\n",
       "      <td>-1.787148</td>\n",
       "      <td>1.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.200140</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>-0.910556</td>\n",
       "      <td>-0.864792</td>\n",
       "      <td>-0.558956</td>\n",
       "      <td>1.281093</td>\n",
       "      <td>-0.052918</td>\n",
       "      <td>0.197674</td>\n",
       "      <td>0.290378</td>\n",
       "      <td>0.510738</td>\n",
       "      <td>-1.818107</td>\n",
       "      <td>1.609438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.200140</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>1.466277</td>\n",
       "      <td>-1.187583</td>\n",
       "      <td>-0.558956</td>\n",
       "      <td>-0.452516</td>\n",
       "      <td>-0.179255</td>\n",
       "      <td>0.837001</td>\n",
       "      <td>-0.722203</td>\n",
       "      <td>-0.263225</td>\n",
       "      <td>-0.333494</td>\n",
       "      <td>4.262680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.200140</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>-1.537740</td>\n",
       "      <td>-0.598246</td>\n",
       "      <td>0.968468</td>\n",
       "      <td>-0.927457</td>\n",
       "      <td>-0.951530</td>\n",
       "      <td>-0.791968</td>\n",
       "      <td>-0.231250</td>\n",
       "      <td>1.965466</td>\n",
       "      <td>0.210747</td>\n",
       "      <td>2.397895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.200140</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>0.260836</td>\n",
       "      <td>-0.344985</td>\n",
       "      <td>0.968468</td>\n",
       "      <td>-0.526058</td>\n",
       "      <td>0.976312</td>\n",
       "      <td>-0.243870</td>\n",
       "      <td>-0.131073</td>\n",
       "      <td>1.953598</td>\n",
       "      <td>-0.814552</td>\n",
       "      <td>2.484907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.200140</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>0.927251</td>\n",
       "      <td>2.314296</td>\n",
       "      <td>-2.321421</td>\n",
       "      <td>-0.775083</td>\n",
       "      <td>0.726297</td>\n",
       "      <td>0.911872</td>\n",
       "      <td>-0.455558</td>\n",
       "      <td>-0.210933</td>\n",
       "      <td>-0.313233</td>\n",
       "      <td>2.197225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.200140</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>0.199831</td>\n",
       "      <td>0.528402</td>\n",
       "      <td>0.968468</td>\n",
       "      <td>-0.910414</td>\n",
       "      <td>-0.352046</td>\n",
       "      <td>-0.213290</td>\n",
       "      <td>-0.975926</td>\n",
       "      <td>-0.687144</td>\n",
       "      <td>0.467966</td>\n",
       "      <td>2.708050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.408857</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>-0.352427</td>\n",
       "      <td>-0.488156</td>\n",
       "      <td>-0.558956</td>\n",
       "      <td>-1.398669</td>\n",
       "      <td>-0.352046</td>\n",
       "      <td>-0.791968</td>\n",
       "      <td>-1.321218</td>\n",
       "      <td>0.774710</td>\n",
       "      <td>-0.113919</td>\n",
       "      <td>2.833213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.408857</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>-8.881784e-16</td>\n",
       "      <td>-0.972938</td>\n",
       "      <td>-0.983449</td>\n",
       "      <td>-0.558956</td>\n",
       "      <td>0.525199</td>\n",
       "      <td>0.151788</td>\n",
       "      <td>-0.964301</td>\n",
       "      <td>0.128349</td>\n",
       "      <td>-1.303884</td>\n",
       "      <td>0.409331</td>\n",
       "      <td>3.258097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Day         Month          Year  Conc. Of  pH        EC    Bicarb  \\\n",
       "0   0.213766  1.110223e-16 -8.881784e-16      0.181918  0.529605  0.937836   \n",
       "1   0.198689  1.110223e-16 -8.881784e-16      0.189731  0.521134  0.922757   \n",
       "2   0.194115  1.110223e-16 -8.881784e-16      0.203582  0.512210  0.953746   \n",
       "3   0.204799  1.110223e-16 -8.881784e-16      0.191872  0.549472  0.960658   \n",
       "4   1.389907  1.110223e-16 -8.881784e-16     -0.409650 -0.473947 -0.564241   \n",
       "5   1.389169  1.110223e-16 -8.881784e-16     -0.326212 -0.507226 -0.535738   \n",
       "6   1.411476  1.110223e-16 -8.881784e-16     -0.329115 -0.501720 -0.586776   \n",
       "7   1.402605  1.110223e-16 -8.881784e-16     -0.344410 -0.471387 -0.567157   \n",
       "8   1.403965  1.110223e-16 -8.881784e-16     -0.961438 -0.985865 -0.578515   \n",
       "9   1.443295  1.110223e-16 -8.881784e-16     -1.030846 -0.980619 -0.570541   \n",
       "10  1.348919  1.110223e-16 -8.881784e-16     -0.987501 -1.009261 -0.564257   \n",
       "11  1.413506  1.110223e-16 -8.881784e-16     -0.980599 -0.977828 -0.536462   \n",
       "12 -1.191048  1.110223e-16 -8.881784e-16      0.742132  0.722244 -0.551269   \n",
       "13 -1.163172  1.110223e-16 -8.881784e-16      0.765892  0.766761 -0.572920   \n",
       "14 -1.148418  1.110223e-16 -8.881784e-16      0.714796  0.699271 -0.543553   \n",
       "15 -1.139837  1.110223e-16 -8.881784e-16      0.772794  0.745214 -0.602549   \n",
       "16  0.216617  1.110223e-16 -8.881784e-16      1.442548 -1.181108 -0.547141   \n",
       "17  0.224547  1.110223e-16 -8.881784e-16      1.524893 -1.220027 -0.610562   \n",
       "18  0.204835  1.110223e-16 -8.881784e-16      1.480022 -1.204236 -0.546412   \n",
       "19  0.176050  1.110223e-16 -8.881784e-16      1.443563 -1.194565 -0.568110   \n",
       "20  0.176482  1.110223e-16 -8.881784e-16      1.484093 -1.150029 -0.490397   \n",
       "21 -1.648966  1.110223e-16 -8.881784e-16     -2.492794  1.620694  0.968468   \n",
       "22 -1.648966  1.110223e-16 -8.881784e-16     -0.910556  0.061813  0.968468   \n",
       "23 -1.648966  1.110223e-16 -8.881784e-16     -1.474684  1.511905  0.968468   \n",
       "24 -1.156934  1.110223e-16 -8.881784e-16     -1.727366  2.376656  2.316229   \n",
       "25 -1.156934  1.110223e-16 -8.881784e-16     -0.972938 -0.380401  0.968468   \n",
       "26 -1.156934  1.110223e-16 -8.881784e-16      0.321770  0.315978  2.316229   \n",
       "27 -1.156934  1.110223e-16 -8.881784e-16      0.806712  0.284894  0.968468   \n",
       "28 -1.156934  1.110223e-16 -8.881784e-16      0.746339  0.703670 -0.558956   \n",
       "29 -0.685845  1.110223e-16 -8.881784e-16     -0.106261  1.169755 -0.558956   \n",
       "30 -0.685845  1.110223e-16 -8.881784e-16     -0.290777  1.090550 -0.558956   \n",
       "31 -0.685845  1.110223e-16 -8.881784e-16      0.199831  1.375127  0.968468   \n",
       "32  0.200140  1.110223e-16 -8.881784e-16      1.466277 -0.710772 -2.321421   \n",
       "33  0.200140  1.110223e-16 -8.881784e-16     -0.414149 -0.983449  0.968468   \n",
       "34  0.200140  1.110223e-16 -8.881784e-16     -0.910556 -0.864792 -0.558956   \n",
       "35  0.200140  1.110223e-16 -8.881784e-16      1.466277 -1.187583 -0.558956   \n",
       "36  0.200140  1.110223e-16 -8.881784e-16     -1.537740 -0.598246  0.968468   \n",
       "37  0.200140  1.110223e-16 -8.881784e-16      0.260836 -0.344985  0.968468   \n",
       "38  0.200140  1.110223e-16 -8.881784e-16      0.927251  2.314296 -2.321421   \n",
       "39  0.200140  1.110223e-16 -8.881784e-16      0.199831  0.528402  0.968468   \n",
       "40  1.408857  1.110223e-16 -8.881784e-16     -0.352427 -0.488156 -0.558956   \n",
       "41  1.408857  1.110223e-16 -8.881784e-16     -0.972938 -0.983449 -0.558956   \n",
       "\n",
       "       Chlor      Iron  Manganese    Sodium  Potassium  Magnesium  Log_Arsenic  \n",
       "0  -0.897475 -0.332173  -0.199134 -0.990882  -0.718107   0.459290     2.712108  \n",
       "1  -0.903200 -0.345305  -0.198272 -0.970444  -0.688951   0.477924     2.704008  \n",
       "2  -0.860840 -0.374221  -0.185931 -0.971685  -0.696098   0.463224     2.671262  \n",
       "3  -0.921168 -0.368752  -0.218919 -0.977194  -0.670931   0.455944     2.705551  \n",
       "4  -1.396437 -0.353426  -0.788518 -1.319128   0.775543  -0.114794     2.811246  \n",
       "5  -1.405846 -0.380960  -0.752810 -1.323950   0.745343  -0.129277     2.864956  \n",
       "6  -1.402391 -0.333524  -0.767888 -1.328753   0.806713  -0.082678     2.827142  \n",
       "7  -1.447269 -0.383242  -0.785012 -1.349649   0.768426  -0.097811     2.847875  \n",
       "8   0.501863  0.153439  -0.944401  0.111024  -1.265145   0.378072     3.250715  \n",
       "9   0.514483  0.135911  -0.975615  0.153014  -1.301238   0.385498     3.235086  \n",
       "10  0.500792  0.116623  -0.960404  0.108902  -1.306280   0.404702     3.283278  \n",
       "11  0.554115  0.160662  -0.945686  0.142984  -1.325308   0.406172     3.241119  \n",
       "12  0.298711 -0.231366   0.086939  1.385282  -0.790186   0.511498     3.298366  \n",
       "13  0.248559 -0.290601   0.124875  1.418864  -0.782977   0.490790     3.230656  \n",
       "14  0.316503 -0.267457   0.089889  1.387848  -0.831066   0.518893     3.259957  \n",
       "15  0.321087 -0.225028   0.104677  1.387150  -0.807854   0.515434     3.249262  \n",
       "16 -0.492377 -0.176832   0.877526 -0.718272  -0.254101  -0.339829     4.245306  \n",
       "17 -0.460338 -0.153983   0.833477 -0.699742  -0.253327  -0.309917     4.265209  \n",
       "18 -0.415926 -0.189666   0.831714 -0.710541  -0.255480  -0.349488     4.247390  \n",
       "19 -0.500286 -0.205625   0.816156 -0.731213  -0.279034  -0.346970     4.236032  \n",
       "20 -0.412671 -0.211854   0.847092 -0.735555  -0.271758  -0.345410     4.255993  \n",
       "21  2.213489  1.216120   0.605988  0.736430   1.313136   1.214296     1.386294  \n",
       "22  1.875330  2.533678   1.498620  0.865038  -0.056871   1.368144     1.945910  \n",
       "23  0.163654  1.911760   3.888941 -0.097930  -0.210933   0.953491     2.079442  \n",
       "24 -1.004502 -1.149010   1.386235 -0.648833   1.196789   1.608201     1.098612  \n",
       "25  0.628290  2.370488  -0.274624  1.795109   0.376643   1.007188     1.609438  \n",
       "26  0.966982 -1.301602  -1.178592  1.986566  -0.483373   1.375757     1.945910  \n",
       "27 -0.131904 -1.099014  -0.431062  2.115548   2.055641   1.411283     1.609438  \n",
       "28  0.300733 -0.265015   0.112192  1.397733  -0.814273   0.510234     3.258097  \n",
       "29  0.361073 -1.405577  -0.152641 -0.172895   1.615090  -2.015707     1.609438  \n",
       "30  0.945695 -0.094729  -1.919036  0.262016  -1.255074  -2.109886     1.098612  \n",
       "31  2.528674 -0.621152  -0.964301  1.150713   1.669452  -2.164018     1.945910  \n",
       "32  1.315407  3.496431   0.054447  0.256990   1.313136  -2.417841     1.791759  \n",
       "33  1.281093 -1.301602   1.542931  0.503957  -0.056871  -1.787148     1.098612  \n",
       "34  1.281093 -0.052918   0.197674  0.290378   0.510738  -1.818107     1.609438  \n",
       "35 -0.452516 -0.179255   0.837001 -0.722203  -0.263225  -0.333494     4.262680  \n",
       "36 -0.927457 -0.951530  -0.791968 -0.231250   1.965466   0.210747     2.397895  \n",
       "37 -0.526058  0.976312  -0.243870 -0.131073   1.953598  -0.814552     2.484907  \n",
       "38 -0.775083  0.726297   0.911872 -0.455558  -0.210933  -0.313233     2.197225  \n",
       "39 -0.910414 -0.352046  -0.213290 -0.975926  -0.687144   0.467966     2.708050  \n",
       "40 -1.398669 -0.352046  -0.791968 -1.321218   0.774710  -0.113919     2.833213  \n",
       "41  0.525199  0.151788  -0.964301  0.128349  -1.303884   0.409331     3.258097  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining The dependent and indepedent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the features and target\n",
    "X = df_final.drop(columns=['Log_Arsenic'])\n",
    "y = df_final['Log_Arsenic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling for the final time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    (\"Linear Regression\", LinearRegression()),\n",
    "    (\"Polynomial Regression\", PolynomialFeatures(degree=2)),  # This will need to be handled specially\n",
    "    (\"Ridge Regression\", Ridge()),\n",
    "    (\"Lasso Regression\", Lasso()),\n",
    "    (\"Elastic Net Regression\", ElasticNet()),\n",
    "    (\"Support Vector Regression\", SVR()),\n",
    "    (\"Decision Tree Regression\", DecisionTreeRegressor()),\n",
    "    (\"Random Forest Regression\", RandomForestRegressor(random_state=42)),\n",
    "    (\"Gradient Boosting Regression\", GradientBoostingRegressor(random_state=42)),\n",
    "    (\"K-Nearest Neighbors Regression\", KNeighborsRegressor()),\n",
    "    (\"Bayesian Regression\", BayesianRidge()),\n",
    "    (\"Neural Network Regression\", MLPRegressor(hidden_layer_sizes=(50,50,50), max_iter=1000, random_state=42)),\n",
    "    (\"Principal Component Regression (PCR)\", make_pipeline(StandardScaler(), PCA(), LinearRegression())),\n",
    "    (\"Partial Least Squares Regression (PLS)\", PLSRegression())\n",
    "]\n",
    "# Define evaluation metrics\n",
    "metrics = {\n",
    "    \"MAE\": mean_absolute_error,\n",
    "    \"MSE\": mean_squared_error,\n",
    "    \"R2\": r2_score\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the k-fold validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Linear Regression...\n",
      "Evaluating Polynomial Regression...\n",
      "Evaluating Ridge Regression...\n",
      "Evaluating Lasso Regression...\n",
      "Evaluating Elastic Net Regression...\n",
      "Evaluating Support Vector Regression...\n",
      "Evaluating Decision Tree Regression...\n",
      "Evaluating Random Forest Regression...\n",
      "Evaluating Gradient Boosting Regression...\n",
      "Evaluating K-Nearest Neighbors Regression...\n",
      "Evaluating Bayesian Regression...\n",
      "Evaluating Neural Network Regression...\n",
      "Evaluating Principal Component Regression (PCR)...\n",
      "Evaluating Partial Least Squares Regression (PLS)...\n",
      "                                        CV_R2_Score           MAE  \\\n",
      "Linear Regression                          0.167647  4.561923e-01   \n",
      "Polynomial Regression                      0.217806  9.128500e-16   \n",
      "Ridge Regression                           0.317766  4.289743e-01   \n",
      "Lasso Regression                          -0.311644  8.133298e-01   \n",
      "Elastic Net Regression                    -0.288568  8.092209e-01   \n",
      "Support Vector Regression                  0.846930  2.720220e-01   \n",
      "Decision Tree Regression                   0.274752  3.861015e-01   \n",
      "Random Forest Regression                   0.634661  4.198173e-01   \n",
      "Gradient Boosting Regression               0.647519  4.061689e-01   \n",
      "K-Nearest Neighbors Regression            -0.032279  4.831339e-01   \n",
      "Bayesian Regression                        0.332541  4.352065e-01   \n",
      "Neural Network Regression                  0.377827  4.236287e-01   \n",
      "Principal Component Regression (PCR)       0.167647  2.062382e-01   \n",
      "Partial Least Squares Regression (PLS)     0.348971  3.848405e-01   \n",
      "\n",
      "                                                 MSE        R2  \n",
      "Linear Regression                       4.337293e-01  0.518584  \n",
      "Polynomial Regression                   1.922848e-30  1.000000  \n",
      "Ridge Regression                        3.494912e-01  0.612083  \n",
      "Lasso Regression                        9.699867e-01 -0.076633  \n",
      "Elastic Net Regression                  9.534373e-01 -0.058264  \n",
      "Support Vector Regression               1.931032e-01  0.785666  \n",
      "Decision Tree Regression                6.057940e-01  0.327601  \n",
      "Random Forest Regression                5.731295e-01  0.363857  \n",
      "Gradient Boosting Regression            6.423270e-01  0.287051  \n",
      "K-Nearest Neighbors Regression          6.104306e-01  0.322455  \n",
      "Bayesian Regression                     3.334634e-01  0.629874  \n",
      "Neural Network Regression               5.357772e-01  0.405316  \n",
      "Principal Component Regression (PCR)    7.345748e-02  0.918466  \n",
      "Partial Least Squares Regression (PLS)  3.102032e-01  0.655691  \n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "# Iterate through the list of models\n",
    "for name, model in models:\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    \n",
    "    if name == \"Polynomial Regression\":\n",
    "        # Special handling for Polynomial Regression\n",
    "        poly = PolynomialFeatures(degree=2)\n",
    "        X_poly = poly.fit_transform(X_scaled)\n",
    "        lr = LinearRegression()\n",
    "        scores = cross_val_score(lr, X_poly, y, cv=kf, scoring='r2')\n",
    "        lr.fit(X_poly, y)\n",
    "        y_pred = lr.predict(poly.transform(X_test))\n",
    "        model = lr  # Update model to be the trained Linear Regression\n",
    "    elif name == \"Principal Component Regression (PCR)\":\n",
    "        # Special handling for PCR\n",
    "        pcr = make_pipeline(StandardScaler(), PCA(), LinearRegression())\n",
    "        scores = cross_val_score(pcr, X_scaled, y, cv=kf, scoring='r2')\n",
    "        pcr.fit(X_scaled, y)\n",
    "        y_pred = pcr.predict(X_test)\n",
    "        model = pcr  # Update model to be the trained pipeline\n",
    "    else:\n",
    "        # Standard handling for other models\n",
    "        scores = cross_val_score(model, X_scaled, y, cv=kf, scoring='r2')\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Store the results\n",
    "    results[name] = {\n",
    "        \"CV_R2_Score\": scores.mean(),\n",
    "        \"MAE\": mae,\n",
    "        \"MSE\": mse,\n",
    "        \"R2\": r2\n",
    "    }\n",
    "\n",
    "    # Save the model\n",
    "    joblib.dump(model, f\"{name}.pkl\")\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv('model_comparison_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here CV R2 Score means the k-fold cross Validation Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CV_R2_Score</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Linear Regression</th>\n",
       "      <td>0.167647</td>\n",
       "      <td>4.561923e-01</td>\n",
       "      <td>4.337293e-01</td>\n",
       "      <td>0.518584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Polynomial Regression</th>\n",
       "      <td>0.217806</td>\n",
       "      <td>9.128500e-16</td>\n",
       "      <td>1.922848e-30</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge Regression</th>\n",
       "      <td>0.317766</td>\n",
       "      <td>4.289743e-01</td>\n",
       "      <td>3.494912e-01</td>\n",
       "      <td>0.612083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso Regression</th>\n",
       "      <td>-0.311644</td>\n",
       "      <td>8.133298e-01</td>\n",
       "      <td>9.699867e-01</td>\n",
       "      <td>-0.076633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Elastic Net Regression</th>\n",
       "      <td>-0.288568</td>\n",
       "      <td>8.092209e-01</td>\n",
       "      <td>9.534373e-01</td>\n",
       "      <td>-0.058264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Vector Regression</th>\n",
       "      <td>0.846930</td>\n",
       "      <td>2.720220e-01</td>\n",
       "      <td>1.931032e-01</td>\n",
       "      <td>0.785666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree Regression</th>\n",
       "      <td>0.258534</td>\n",
       "      <td>4.560451e-01</td>\n",
       "      <td>7.352958e-01</td>\n",
       "      <td>0.183861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest Regression</th>\n",
       "      <td>0.634661</td>\n",
       "      <td>4.198173e-01</td>\n",
       "      <td>5.731295e-01</td>\n",
       "      <td>0.363857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting Regression</th>\n",
       "      <td>0.647519</td>\n",
       "      <td>4.061689e-01</td>\n",
       "      <td>6.423270e-01</td>\n",
       "      <td>0.287051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K-Nearest Neighbors Regression</th>\n",
       "      <td>-0.032279</td>\n",
       "      <td>4.831339e-01</td>\n",
       "      <td>6.104306e-01</td>\n",
       "      <td>0.322455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bayesian Regression</th>\n",
       "      <td>0.332541</td>\n",
       "      <td>4.352065e-01</td>\n",
       "      <td>3.334634e-01</td>\n",
       "      <td>0.629874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Network Regression</th>\n",
       "      <td>0.377827</td>\n",
       "      <td>4.236287e-01</td>\n",
       "      <td>5.357772e-01</td>\n",
       "      <td>0.405316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Principal Component Regression (PCR)</th>\n",
       "      <td>0.167647</td>\n",
       "      <td>2.062382e-01</td>\n",
       "      <td>7.345748e-02</td>\n",
       "      <td>0.918466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Partial Least Squares Regression (PLS)</th>\n",
       "      <td>0.348971</td>\n",
       "      <td>3.848405e-01</td>\n",
       "      <td>3.102032e-01</td>\n",
       "      <td>0.655691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        CV_R2_Score           MAE  \\\n",
       "Linear Regression                          0.167647  4.561923e-01   \n",
       "Polynomial Regression                      0.217806  9.128500e-16   \n",
       "Ridge Regression                           0.317766  4.289743e-01   \n",
       "Lasso Regression                          -0.311644  8.133298e-01   \n",
       "Elastic Net Regression                    -0.288568  8.092209e-01   \n",
       "Support Vector Regression                  0.846930  2.720220e-01   \n",
       "Decision Tree Regression                   0.258534  4.560451e-01   \n",
       "Random Forest Regression                   0.634661  4.198173e-01   \n",
       "Gradient Boosting Regression               0.647519  4.061689e-01   \n",
       "K-Nearest Neighbors Regression            -0.032279  4.831339e-01   \n",
       "Bayesian Regression                        0.332541  4.352065e-01   \n",
       "Neural Network Regression                  0.377827  4.236287e-01   \n",
       "Principal Component Regression (PCR)       0.167647  2.062382e-01   \n",
       "Partial Least Squares Regression (PLS)     0.348971  3.848405e-01   \n",
       "\n",
       "                                                 MSE        R2  \n",
       "Linear Regression                       4.337293e-01  0.518584  \n",
       "Polynomial Regression                   1.922848e-30  1.000000  \n",
       "Ridge Regression                        3.494912e-01  0.612083  \n",
       "Lasso Regression                        9.699867e-01 -0.076633  \n",
       "Elastic Net Regression                  9.534373e-01 -0.058264  \n",
       "Support Vector Regression               1.931032e-01  0.785666  \n",
       "Decision Tree Regression                7.352958e-01  0.183861  \n",
       "Random Forest Regression                5.731295e-01  0.363857  \n",
       "Gradient Boosting Regression            6.423270e-01  0.287051  \n",
       "K-Nearest Neighbors Regression          6.104306e-01  0.322455  \n",
       "Bayesian Regression                     3.334634e-01  0.629874  \n",
       "Neural Network Regression               5.357772e-01  0.405316  \n",
       "Principal Component Regression (PCR)    7.345748e-02  0.918466  \n",
       "Partial Least Squares Regression (PLS)  3.102032e-01  0.655691  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OutPut Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations:\n",
    " ### Polynomial Regression:\n",
    "\n",
    "R2: 1.0, indicating a perfect fit on the test data.\n",
    "MAE & MSE: Extremely low values (close to zero), indicating very accurate predictions.\n",
    "Note: This may suggest overfitting, especially if the cross-validation score is not similarly high.\n",
    "\n",
    " ### Support Vector Regression (SVR):\n",
    "\n",
    "CV_R2_Score: 0.847, the highest among all models.\n",
    "R2: 0.786, indicating strong performance on the test data.\n",
    "MAE & MSE: Low values, indicating good prediction accuracy.\n",
    "\n",
    " ### Ridge Regression:\n",
    "\n",
    "CV_R2_Score: 0.318, better than simple linear regression.\n",
    "R2: 0.612, indicating decent performance on the test data.\n",
    "MAE & MSE: Improved over linear regression.\n",
    "\n",
    " ### Lasso and Elastic Net Regression:\n",
    "\n",
    "CV_R2_Score: Negative values, indicating poor performance during cross-validation.\n",
    "R2: Negative and close to zero, indicating poor performance on the test data.\n",
    "\n",
    " ### Random Forest Regression:\n",
    "\n",
    "CV_R2_Score: 0.635, indicating good performance during cross-validation.\n",
    "R2: 0.364, lower than expected performance on the test data.\n",
    "\n",
    " ### Gradient Boosting Regression:\n",
    "\n",
    "CV_R2_Score: 0.648, indicating good performance during cross-validation.\n",
    "R2: 0.287, indicating moderate performance on the test data.\n",
    " \n",
    " ### Principal Component Regression (PCR):\n",
    "\n",
    "R2: 0.918, indicating very strong performance on the test data.\n",
    "CV_R2_Score: 0.168, indicating potential overfitting.\n",
    " \n",
    " ### K-Nearest Neighbors Regression:\n",
    "\n",
    "CV_R2_Score: Negative value, indicating poor performance during cross-validation.\n",
    "R2: 0.322, indicating moderate performance on the test data.\n",
    " \n",
    " ### Neural Network Regression:\n",
    "\n",
    "CV_R2_Score: 0.378, indicating decent performance during cross-validation.\n",
    "R2: 0.405, indicating decent performance on the test data.\n",
    " \n",
    " ### Bayesian Regression:\n",
    "\n",
    "CV_R2_Score: 0.333, indicating decent performance during cross-validation.\n",
    "R2: 0.630, indicating good performance on the test data.\n",
    " \n",
    " ### Partial Least Squares Regression (PLS):\n",
    "\n",
    "CV_R2_Score: 0.349, indicating decent performance during cross-validation.\n",
    "R2: 0.656, indicating good performance on the test data.\n",
    " \n",
    "## Conclusion:\n",
    "\n",
    " ### Best Performing Models:\n",
    "\n",
    "Polynomial Regression and Support Vector Regression (SVR) show the best performance on the test data with very high R scores.\n",
    "PCR also shows a high R score, but its cross-validation score suggests potential overfitting.\n",
    "\n",
    " ### Moderately Performing Models:\n",
    "\n",
    "Ridge Regression, Bayesian Regression, and PLS show good performance with reasonable R scores.\n",
    "Poor Performing Models:\n",
    "\n",
    "Lasso Regression, Elastic Net Regression, and K-Nearest Neighbors Regression show poor performance with negative or low R scores.\n",
    "\n",
    "### Caution with Overfitting:\n",
    "\n",
    "Polynomial Regression shows perfect fit on the test data but might be overfitting, as indicated by the disparity between its cross-validation score and test score.\n",
    "PCR shows high test performance but much lower cross-validation performance, suggesting overfitting.\n",
    "Given the results, Support Vector Regression (SVR) appears to be the most reliable model with consistently high performance across both cross-validation and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Cross Validation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Support Vector Regression (SVR):\n",
    "\n",
    "CV_R2_Score: 0.846930 (high cross-validation score, indicating strong performance across folds)\n",
    "R2: 0.785666 (high test score, indicating consistent performance on unseen data)\n",
    "\n",
    "## Ridge Regression:\n",
    "\n",
    "CV_R2_Score: 0.317766 (moderate cross-validation score)\n",
    "R2: 0.612083 (good test score, indicating improved performance over linear regression)\n",
    "\n",
    "## Polynomial Regression:\n",
    "\n",
    "CV_R2_Score: 0.217806 (moderate cross-validation score, indicating potential overfitting)\n",
    "R2: 1.000000 (perfect test score, likely overfitting)\n",
    "\n",
    "## Principal Component Regression (PCR):\n",
    "\n",
    "CV_R2_Score: 0.167647 (moderate cross-validation score)\n",
    "R2: 0.918466 (high test score, indicating potential overfitting)\n",
    "\n",
    "## Recommendations:\n",
    "\n",
    "High Performing Models: SVR, Ridge Regression, and Bayesian Regression are strong candidates for further tuning and deployment due to their consistent performance.\n",
    "\n",
    "Overfitting Concerns: Polynomial Regression and PCR show signs of overfitting despite their high test scores. These models should be used cautiously.\n",
    "\n",
    "Further Tuning: Additional hyperparameter tuning and feature engineering could improve the performance of models like Random Forest and Gradient Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will use GridSearchCV to perform hyperparameter tuning for each of these models.\n",
    "\n",
    "Hyperparameter Tuning for Top 3 Models\n",
    "1. Support Vector Regression (SVR)\n",
    "2. Ridge Regression\n",
    "3. Bayesian Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge, BayesianRidge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter List for the Model which will be tuned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform\n",
    "# Define hyperparameter grids for each model\n",
    "\n",
    "param_grid_svr = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid']\n",
    "}\n",
    "\n",
    "param_grid_ridge = {\n",
    "    'alpha': [0.1, 1, 10, 100],\n",
    "    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sag']\n",
    "}\n",
    "\n",
    "param_grid_bayesian = {\n",
    "    'n_iter': [100, 200, 300, 400, 500],\n",
    "    'alpha_1': [1e-6, 1e-5, 1e-4],\n",
    "    'alpha_2': [1e-6, 1e-5, 1e-4],\n",
    "    'lambda_1': [1e-6, 1e-5, 1e-4],\n",
    "    'lambda_2': [1e-6, 1e-5, 1e-4]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge, BayesianRidge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "svr = SVR()\n",
    "ridge = Ridge()  # Not `RidgeRegression`\n",
    "bayesian = BayesianRidge()  # N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing the Grid Search CV to find the best parameter for the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform grid search for each model\n",
    "grid_svr = GridSearchCV(svr, param_grid_svr, cv=5, scoring='r2', n_jobs=-1, verbose=2)\n",
    "grid_ridge = GridSearchCV(ridge, param_grid_ridge, cv=5, scoring='r2', n_jobs=-1, verbose=2)\n",
    "grid_bayesian = GridSearchCV(bayesian, param_grid_bayesian, cv=5, scoring='r2', n_jobs=-1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 405 candidates, totalling 2025 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter 'n_iter' for estimator BayesianRidge(). Valid parameters are: ['alpha_1', 'alpha_2', 'alpha_init', 'compute_score', 'copy_X', 'fit_intercept', 'lambda_1', 'lambda_2', 'lambda_init', 'max_iter', 'tol', 'verbose'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\Nitro Gaming\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 463, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"c:\\Users\\Nitro Gaming\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Nitro Gaming\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Nitro Gaming\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 129, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Nitro Gaming\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 876, in _fit_and_score\n    estimator = estimator.set_params(**clone(parameters, safe=False))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Nitro Gaming\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 279, in set_params\n    raise ValueError(\nValueError: Invalid parameter 'n_iter' for estimator BayesianRidge(). Valid parameters are: ['alpha_1', 'alpha_2', 'alpha_init', 'compute_score', 'copy_X', 'fit_intercept', 'lambda_1', 'lambda_2', 'lambda_init', 'max_iter', 'tol', 'verbose'].\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m grid_svr\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m      3\u001b[0m grid_ridge\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mgrid_bayesian\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nitro Gaming\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nitro Gaming\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:968\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    962\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    963\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    964\u001b[0m     )\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 968\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    972\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Nitro Gaming\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1543\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1541\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1542\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1543\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nitro Gaming\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:914\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    907\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    908\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    909\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    910\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    911\u001b[0m         )\n\u001b[0;32m    912\u001b[0m     )\n\u001b[1;32m--> 914\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    933\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    934\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    935\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    937\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Nitro Gaming\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nitro Gaming\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nitro Gaming\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Nitro Gaming\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1748\u001b[0m \n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m-> 1754\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Nitro Gaming\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediatly raise the error by\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1789\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nitro Gaming\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[1;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Nitro Gaming\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid parameter 'n_iter' for estimator BayesianRidge(). Valid parameters are: ['alpha_1', 'alpha_2', 'alpha_init', 'compute_score', 'copy_X', 'fit_intercept', 'lambda_1', 'lambda_2', 'lambda_init', 'max_iter', 'tol', 'verbose']."
     ]
    }
   ],
   "source": [
    "# Fit grid searches\n",
    "grid_svr.fit(X_train, y_train)\n",
    "grid_ridge.fit(X_train, y_train)\n",
    "grid_bayesian.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for SVR: {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "Best cross-validation R score for SVR: 0.8411548315844988\n",
      "Best parameters for Ridge: {'alpha': 10, 'solver': 'lsqr'}\n",
      "Best cross-validation R score for Ridge: 0.3074171887310845\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_params_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters for Ridge:\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_ridge\u001b[38;5;241m.\u001b[39mbest_params_)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest cross-validation R score for Ridge:\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_ridge\u001b[38;5;241m.\u001b[39mbest_score_)\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters for Bayesian:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mgrid_bayesian\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_params_\u001b[49m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest cross-validation R score for Bayesian:\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_bayesian\u001b[38;5;241m.\u001b[39mbest_score_)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_params_'"
     ]
    }
   ],
   "source": [
    "# Print best parameters and scores for each model\n",
    "print(\"Best parameters for SVR:\", grid_svr.best_params_)\n",
    "print(\"Best cross-validation R score for SVR:\", grid_svr.best_score_)\n",
    "\n",
    "print(\"Best parameters for Ridge:\", grid_ridge.best_params_)\n",
    "print(\"Best cross-validation R score for Ridge:\", grid_ridge.best_score_)\n",
    "\n",
    "print(\"Best parameters for Bayesian:\", grid_bayesian.best_params_)\n",
    "print(\"Best cross-validation R score for Bayesian:\", grid_bayesian.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now using the best parameters to tune the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               MAE       MSE        R2\n",
      "SVR       0.269002  0.199971  0.778042\n",
      "Ridge     0.466373  0.358771  0.601783\n",
      "Bayesian  0.435209  0.333465  0.629872\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best models on the test set\n",
    "best_svr = grid_svr.best_estimator_\n",
    "best_ridge = grid_ridge.best_estimator_\n",
    "best_bayesian = grid_bayesian.best_estimator_\n",
    "\n",
    "models = [(\"SVR\", best_svr), (\"Ridge\", best_ridge), (\"Bayesian\", best_bayesian)]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models:\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Store the results\n",
    "    results[name] = {\n",
    "        \"MAE\": mae,\n",
    "        \"MSE\": mse,\n",
    "        \"R2\": r2\n",
    "    }\n",
    "\n",
    "    # Save the model\n",
    "    joblib.dump(model, f\"{name}_best_model.pkl\")\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv('hypertuned_model_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR with Polynomial Features - MAE: 0.28470374674347354, MSE: 0.2071481868718464, R2: 0.7700766161838215\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Generate polynomial features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X_scaled)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(X_poly, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Evaluate with SVR\n",
    "svr_poly = SVR(C=10, gamma='scale', kernel='rbf')  # Using best params from previous tuning\n",
    "svr_poly.fit(X_train_poly, y_train_poly)\n",
    "y_pred_poly = svr_poly.predict(X_test_poly)\n",
    "\n",
    "# Calculate metrics\n",
    "mae_poly = mean_absolute_error(y_test_poly, y_pred_poly)\n",
    "mse_poly = mean_squared_error(y_test_poly, y_pred_poly)\n",
    "r2_poly = r2_score(y_test_poly, y_pred_poly)\n",
    "\n",
    "print(f\"SVR with Polynomial Features - MAE: {mae_poly}, MSE: {mse_poly}, R2: {r2_poly}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Ensembled Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Model - MAE: 0.3805426571051523, MSE: 0.5474853138676409, R2: 0.39232064805864386\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Define and train XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(f\"XGBoost Model - MAE: {mae_xgb}, MSE: {mse_xgb}, R2: {r2_xgb}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Models (Ensemble )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation MAE scores for each fold: [0.22542481 0.16927933 0.28900586 0.30879518 0.14519045]\n",
      "Cross-validation MSE scores for each fold: [0.10238496 0.06096286 0.13254362 0.12942435 0.02374588]\n",
      "Cross-validation R2 scores for each fold: [0.87587693 0.67114169 0.90873866 0.76880754 0.96013668]\n",
      "Mean cross-validated MAE: 0.22753912841317528\n",
      "Mean cross-validated MSE: 0.08981233463984364\n",
      "Mean cross-validated R2: 0.8369403000290113\n",
      "Test Set - Stacking Model - MAE: 0.21034456500762985, MSE: 0.09506965002700721, R2: 0.8944777844183961\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Define base models\n",
    "base_models = [\n",
    "    ('ridge', Ridge(alpha=10)),\n",
    "    ('svr', SVR(C=10, gamma='scale', kernel='rbf')),\n",
    "    ('bayesian', BayesianRidge())\n",
    "]\n",
    "\n",
    "# Define stacking model\n",
    "stacking_model = StackingRegressor(\n",
    "    estimators=base_models,\n",
    "    final_estimator=LinearRegression()\n",
    ")\n",
    "\n",
    "# Define KFold cross-validator\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Evaluate model using cross-validation\n",
    "cv_mae_scores = cross_val_score(stacking_model, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
    "cv_mse_scores = cross_val_score(stacking_model, X_train, y_train, cv=kf, scoring='neg_mean_squared_error')\n",
    "cv_r2_scores = cross_val_score(stacking_model, X_train, y_train, cv=kf, scoring='r2')\n",
    "\n",
    "# Print cross-validation results for each fold\n",
    "print(\"Cross-validation MAE scores for each fold:\", -cv_mae_scores)\n",
    "print(\"Cross-validation MSE scores for each fold:\", -cv_mse_scores)\n",
    "print(\"Cross-validation R2 scores for each fold:\", cv_r2_scores)\n",
    "\n",
    "# Print mean cross-validation scores\n",
    "print(f\"Mean cross-validated MAE: {-cv_mae_scores.mean()}\")\n",
    "print(f\"Mean cross-validated MSE: {-cv_mse_scores.mean()}\")\n",
    "print(f\"Mean cross-validated R2: {cv_r2_scores.mean()}\")\n",
    "\n",
    "# Train stacking model on full training set\n",
    "stacking_model.fit(X_train, y_train)\n",
    "y_pred_stack = stacking_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics on test set\n",
    "mae_stack = mean_absolute_error(y_test, y_pred_stack)\n",
    "mse_stack = mean_squared_error(y_test, y_pred_stack)\n",
    "r2_stack = r2_score(y_test, y_pred_stack)\n",
    "\n",
    "print(f\"Test Set - Stacking Model - MAE: {mae_stack}, MSE: {mse_stack}, R2: {r2_stack}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Experiment with Ensemble Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, BayesianRidge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "import joblib\n",
    "\n",
    "# Load the final feature engineered data\n",
    "df_final = pd.read_csv('final_feature_engineered_data.csv')\n",
    "\n",
    "# Separate the features and target\n",
    "X = df_final.drop(columns=['Log_Arsenic'])\n",
    "y = df_final['Log_Arsenic']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a list of base models\n",
    "base_models = [\n",
    "    ('ridge', Ridge(alpha=10)),\n",
    "    ('svr', SVR(C=10, gamma='scale', kernel='rbf')),\n",
    "    ('bayesian', BayesianRidge()),\n",
    "    ('rf', RandomForestRegressor(random_state=42)),\n",
    "    ('gb', GradientBoostingRegressor(random_state=42)),\n",
    "    ('knn', KNeighborsRegressor())\n",
    "]\n",
    "\n",
    "# Define a list of meta-models\n",
    "meta_models = [\n",
    "    ('linear', LinearRegression()),\n",
    "    ('ridge', Ridge(alpha=10)),\n",
    "    ('lasso', Lasso(alpha=0.1)),\n",
    "    ('elasticnet', ElasticNet(alpha=0.1))\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    CV_Mean_R2  CV_Std_R2  \\\n",
      "STACKING_LINEAR_WITH_RIDGE                            0.081148   0.392351   \n",
      "STACKING_LINEAR_WITH_SVR                              0.820242   0.140375   \n",
      "STACKING_LINEAR_WITH_BAYESIAN                         0.013654   0.257066   \n",
      "STACKING_LINEAR_WITH_RF                               0.741289   0.136416   \n",
      "STACKING_LINEAR_WITH_GB                               0.740399   0.222841   \n",
      "...                                                        ...        ...   \n",
      "STACKING_ELASTICNET_WITH_RIDGE_SVR_BAYESIAN_GB_KNN    0.818225   0.097971   \n",
      "STACKING_ELASTICNET_WITH_RIDGE_SVR_RF_GB_KNN          0.838978   0.088808   \n",
      "STACKING_ELASTICNET_WITH_RIDGE_BAYESIAN_RF_GB_KNN     0.771861   0.140312   \n",
      "STACKING_ELASTICNET_WITH_SVR_BAYESIAN_RF_GB_KNN       0.818597   0.097804   \n",
      "STACKING_ELASTICNET_WITH_RIDGE_SVR_BAYESIAN_RF_...    0.818279   0.097955   \n",
      "\n",
      "                                                         MAE       MSE  \\\n",
      "STACKING_LINEAR_WITH_RIDGE                          0.491528  0.381131   \n",
      "STACKING_LINEAR_WITH_SVR                            0.202634  0.105406   \n",
      "STACKING_LINEAR_WITH_BAYESIAN                       0.618847  0.556424   \n",
      "STACKING_LINEAR_WITH_RF                             0.381499  0.481085   \n",
      "STACKING_LINEAR_WITH_GB                             0.397857  0.615138   \n",
      "...                                                      ...       ...   \n",
      "STACKING_ELASTICNET_WITH_RIDGE_SVR_BAYESIAN_GB_KNN  0.316764  0.303450   \n",
      "STACKING_ELASTICNET_WITH_RIDGE_SVR_RF_GB_KNN        0.319557  0.317061   \n",
      "STACKING_ELASTICNET_WITH_RIDGE_BAYESIAN_RF_GB_KNN   0.420550  0.555156   \n",
      "STACKING_ELASTICNET_WITH_SVR_BAYESIAN_RF_GB_KNN     0.319550  0.317048   \n",
      "STACKING_ELASTICNET_WITH_RIDGE_SVR_BAYESIAN_RF_...  0.319552  0.317055   \n",
      "\n",
      "                                                          R2  \n",
      "STACKING_LINEAR_WITH_RIDGE                          0.576965  \n",
      "STACKING_LINEAR_WITH_SVR                            0.883005  \n",
      "STACKING_LINEAR_WITH_BAYESIAN                       0.382399  \n",
      "STACKING_LINEAR_WITH_RF                             0.466021  \n",
      "STACKING_LINEAR_WITH_GB                             0.317230  \n",
      "...                                                      ...  \n",
      "STACKING_ELASTICNET_WITH_RIDGE_SVR_BAYESIAN_GB_KNN  0.663186  \n",
      "STACKING_ELASTICNET_WITH_RIDGE_SVR_RF_GB_KNN        0.648079  \n",
      "STACKING_ELASTICNET_WITH_RIDGE_BAYESIAN_RF_GB_KNN   0.383807  \n",
      "STACKING_ELASTICNET_WITH_SVR_BAYESIAN_RF_GB_KNN     0.648094  \n",
      "STACKING_ELASTICNET_WITH_RIDGE_SVR_BAYESIAN_RF_...  0.648086  \n",
      "\n",
      "[252 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from sklearn.ensemble import StackingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, BayesianRidge, LinearRegression, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Define a list of base models\n",
    "base_models = [\n",
    "    ('ridge', Ridge(alpha=10)),\n",
    "    ('svr', SVR(C=10, gamma='scale', kernel='rbf')),\n",
    "    ('bayesian', BayesianRidge()),\n",
    "    ('rf', RandomForestRegressor(random_state=42)),\n",
    "    ('gb', GradientBoostingRegressor(random_state=42)),\n",
    "    ('knn', KNeighborsRegressor())\n",
    "]\n",
    "\n",
    "# Define a list of meta-models\n",
    "meta_models = [\n",
    "    ('linear', LinearRegression()),\n",
    "    ('ridge', Ridge(alpha=10)),\n",
    "    ('lasso', Lasso(alpha=0.1)),\n",
    "    ('elasticnet', ElasticNet(alpha=0.1))\n",
    "]\n",
    "\n",
    "# Dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "# Define k-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Iterate through each combination of base models and meta-models\n",
    "for meta_name, meta_model in meta_models:\n",
    "    for i in range(1, len(base_models) + 1):\n",
    "        for base_combination in itertools.combinations(base_models, i):\n",
    "            # Ensure base_combination is a list\n",
    "            base_combination_list = list(base_combination)\n",
    "            \n",
    "            # Define the stacking model\n",
    "            stacking_model = StackingRegressor(\n",
    "                estimators=base_combination_list,\n",
    "                final_estimator=meta_model,\n",
    "                cv=kf,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "            # Perform k-fold cross-validation\n",
    "            cv_results = cross_val_score(stacking_model, X_train, y_train, cv=kf, scoring='r2', n_jobs=-1)\n",
    "            mean_cv_r2 = cv_results.mean()\n",
    "            std_cv_r2 = cv_results.std()\n",
    "            \n",
    "            # Train the stacking model\n",
    "            stacking_model.fit(X_train, y_train)\n",
    "            y_pred_stack = stacking_model.predict(X_test)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mae_stack = mean_absolute_error(y_test, y_pred_stack)\n",
    "            mse_stack = mean_squared_error(y_test, y_pred_stack)\n",
    "            r2_stack = r2_score(y_test, y_pred_stack)\n",
    "            \n",
    "            # Store the results\n",
    "            model_name = f\"STACKING_{meta_name.upper()}_WITH_{'_'.join([name.upper() for name, _ in base_combination])}\"\n",
    "            results[model_name] = {\n",
    "                \"CV_Mean_R2\": mean_cv_r2,\n",
    "                \"CV_Std_R2\": std_cv_r2,\n",
    "                \"MAE\": mae_stack,\n",
    "                \"MSE\": mse_stack,\n",
    "                \"R2\": r2_stack\n",
    "            }\n",
    "\n",
    "            # Save the model\n",
    "            joblib.dump(stacking_model, f\"{model_name}.pkl\")\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv('ensemble_model_comparison_results.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CV_Mean_R2</th>\n",
       "      <th>CV_Std_R2</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>STACKING_LINEAR_WITH_RIDGE</th>\n",
       "      <td>0.081148</td>\n",
       "      <td>0.392351</td>\n",
       "      <td>0.491528</td>\n",
       "      <td>0.381131</td>\n",
       "      <td>0.576965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STACKING_LINEAR_WITH_SVR</th>\n",
       "      <td>0.820242</td>\n",
       "      <td>0.140375</td>\n",
       "      <td>0.202634</td>\n",
       "      <td>0.105406</td>\n",
       "      <td>0.883005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STACKING_LINEAR_WITH_BAYESIAN</th>\n",
       "      <td>0.013654</td>\n",
       "      <td>0.257066</td>\n",
       "      <td>0.618847</td>\n",
       "      <td>0.556424</td>\n",
       "      <td>0.382399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STACKING_LINEAR_WITH_RF</th>\n",
       "      <td>0.741289</td>\n",
       "      <td>0.136416</td>\n",
       "      <td>0.381499</td>\n",
       "      <td>0.481085</td>\n",
       "      <td>0.466021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STACKING_LINEAR_WITH_GB</th>\n",
       "      <td>0.740399</td>\n",
       "      <td>0.222841</td>\n",
       "      <td>0.397857</td>\n",
       "      <td>0.615138</td>\n",
       "      <td>0.317230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STACKING_ELASTICNET_WITH_RIDGE_SVR_BAYESIAN_GB_KNN</th>\n",
       "      <td>0.818225</td>\n",
       "      <td>0.097971</td>\n",
       "      <td>0.316764</td>\n",
       "      <td>0.303450</td>\n",
       "      <td>0.663186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STACKING_ELASTICNET_WITH_RIDGE_SVR_RF_GB_KNN</th>\n",
       "      <td>0.838978</td>\n",
       "      <td>0.088808</td>\n",
       "      <td>0.319557</td>\n",
       "      <td>0.317061</td>\n",
       "      <td>0.648079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STACKING_ELASTICNET_WITH_RIDGE_BAYESIAN_RF_GB_KNN</th>\n",
       "      <td>0.771861</td>\n",
       "      <td>0.140312</td>\n",
       "      <td>0.420550</td>\n",
       "      <td>0.555156</td>\n",
       "      <td>0.383807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STACKING_ELASTICNET_WITH_SVR_BAYESIAN_RF_GB_KNN</th>\n",
       "      <td>0.818597</td>\n",
       "      <td>0.097804</td>\n",
       "      <td>0.319550</td>\n",
       "      <td>0.317048</td>\n",
       "      <td>0.648094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STACKING_ELASTICNET_WITH_RIDGE_SVR_BAYESIAN_RF_GB_KNN</th>\n",
       "      <td>0.818279</td>\n",
       "      <td>0.097955</td>\n",
       "      <td>0.319552</td>\n",
       "      <td>0.317055</td>\n",
       "      <td>0.648086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    CV_Mean_R2  CV_Std_R2  \\\n",
       "STACKING_LINEAR_WITH_RIDGE                            0.081148   0.392351   \n",
       "STACKING_LINEAR_WITH_SVR                              0.820242   0.140375   \n",
       "STACKING_LINEAR_WITH_BAYESIAN                         0.013654   0.257066   \n",
       "STACKING_LINEAR_WITH_RF                               0.741289   0.136416   \n",
       "STACKING_LINEAR_WITH_GB                               0.740399   0.222841   \n",
       "...                                                        ...        ...   \n",
       "STACKING_ELASTICNET_WITH_RIDGE_SVR_BAYESIAN_GB_KNN    0.818225   0.097971   \n",
       "STACKING_ELASTICNET_WITH_RIDGE_SVR_RF_GB_KNN          0.838978   0.088808   \n",
       "STACKING_ELASTICNET_WITH_RIDGE_BAYESIAN_RF_GB_KNN     0.771861   0.140312   \n",
       "STACKING_ELASTICNET_WITH_SVR_BAYESIAN_RF_GB_KNN       0.818597   0.097804   \n",
       "STACKING_ELASTICNET_WITH_RIDGE_SVR_BAYESIAN_RF_...    0.818279   0.097955   \n",
       "\n",
       "                                                         MAE       MSE  \\\n",
       "STACKING_LINEAR_WITH_RIDGE                          0.491528  0.381131   \n",
       "STACKING_LINEAR_WITH_SVR                            0.202634  0.105406   \n",
       "STACKING_LINEAR_WITH_BAYESIAN                       0.618847  0.556424   \n",
       "STACKING_LINEAR_WITH_RF                             0.381499  0.481085   \n",
       "STACKING_LINEAR_WITH_GB                             0.397857  0.615138   \n",
       "...                                                      ...       ...   \n",
       "STACKING_ELASTICNET_WITH_RIDGE_SVR_BAYESIAN_GB_KNN  0.316764  0.303450   \n",
       "STACKING_ELASTICNET_WITH_RIDGE_SVR_RF_GB_KNN        0.319557  0.317061   \n",
       "STACKING_ELASTICNET_WITH_RIDGE_BAYESIAN_RF_GB_KNN   0.420550  0.555156   \n",
       "STACKING_ELASTICNET_WITH_SVR_BAYESIAN_RF_GB_KNN     0.319550  0.317048   \n",
       "STACKING_ELASTICNET_WITH_RIDGE_SVR_BAYESIAN_RF_...  0.319552  0.317055   \n",
       "\n",
       "                                                          R2  \n",
       "STACKING_LINEAR_WITH_RIDGE                          0.576965  \n",
       "STACKING_LINEAR_WITH_SVR                            0.883005  \n",
       "STACKING_LINEAR_WITH_BAYESIAN                       0.382399  \n",
       "STACKING_LINEAR_WITH_RF                             0.466021  \n",
       "STACKING_LINEAR_WITH_GB                             0.317230  \n",
       "...                                                      ...  \n",
       "STACKING_ELASTICNET_WITH_RIDGE_SVR_BAYESIAN_GB_KNN  0.663186  \n",
       "STACKING_ELASTICNET_WITH_RIDGE_SVR_RF_GB_KNN        0.648079  \n",
       "STACKING_ELASTICNET_WITH_RIDGE_BAYESIAN_RF_GB_KNN   0.383807  \n",
       "STACKING_ELASTICNET_WITH_SVR_BAYESIAN_RF_GB_KNN     0.648094  \n",
       "STACKING_ELASTICNET_WITH_RIDGE_SVR_BAYESIAN_RF_...  0.648086  \n",
       "\n",
       "[252 rows x 5 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select the best ensemble model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model based on test R score:\n",
      "Unnamed: 0    STACKING_LINEAR_WITH_SVR_RF\n",
      "CV_Mean_R2                       0.794076\n",
      "CV_Std_R2                        0.183497\n",
      "MAE                              0.182704\n",
      "MSE                              0.073791\n",
      "R2                               0.918096\n",
      "Name: 12, dtype: object\n",
      "\n",
      "Best Model based on Cross-Validation Mean R score:\n",
      "Unnamed: 0    STACKING_LINEAR_WITH_SVR_KNN\n",
      "CV_Mean_R2                        0.879771\n",
      "CV_Std_R2                          0.10028\n",
      "MAE                               0.265883\n",
      "MSE                               0.124153\n",
      "R2                                0.862197\n",
      "Name: 14, dtype: object\n",
      "\n",
      "Top 5 Models based on test R score:\n",
      "                                       Unnamed: 0  CV_Mean_R2  CV_Std_R2  \\\n",
      "12                    STACKING_LINEAR_WITH_SVR_RF    0.794076   0.183497   \n",
      "41     STACKING_LINEAR_WITH_RIDGE_SVR_BAYESIAN_RF    0.639876   0.283577   \n",
      "56  STACKING_LINEAR_WITH_RIDGE_SVR_BAYESIAN_RF_GB    0.404885   0.680023   \n",
      "31           STACKING_LINEAR_WITH_SVR_BAYESIAN_RF    0.728839   0.282440   \n",
      "34                 STACKING_LINEAR_WITH_SVR_RF_GB    0.745078   0.201218   \n",
      "\n",
      "         MAE       MSE        R2  \n",
      "12  0.182704  0.073791  0.918096  \n",
      "41  0.198073  0.078027  0.913394  \n",
      "56  0.192374  0.087071  0.903356  \n",
      "31  0.213909  0.089995  0.900110  \n",
      "34  0.181319  0.091967  0.897922  \n",
      "\n",
      "Top 5 Models based on Cross-Validation Mean R score:\n",
      "                                 Unnamed: 0  CV_Mean_R2  CV_Std_R2       MAE  \\\n",
      "14             STACKING_LINEAR_WITH_SVR_KNN    0.879771   0.100280  0.265883   \n",
      "202         STACKING_ELASTICNET_WITH_SVR_GB    0.845409   0.089937  0.316762   \n",
      "225     STACKING_ELASTICNET_WITH_SVR_GB_KNN    0.845409   0.089937  0.316762   \n",
      "243  STACKING_ELASTICNET_WITH_SVR_RF_GB_KNN    0.845243   0.089816  0.319550   \n",
      "223      STACKING_ELASTICNET_WITH_SVR_RF_GB    0.845243   0.089816  0.319550   \n",
      "\n",
      "          MSE        R2  \n",
      "14   0.124153  0.862197  \n",
      "202  0.303447  0.663190  \n",
      "225  0.303447  0.663190  \n",
      "243  0.317048  0.648094  \n",
      "223  0.317048  0.648094  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the results DataFrame\n",
    "results_df = pd.read_csv('ensemble_model_comparison_results.csv')\n",
    "\n",
    "# Select the best model based on test R score\n",
    "best_models_r2 = results_df.sort_values(by='R2', ascending=False)\n",
    "best_model_r2 = best_models_r2.iloc[0]\n",
    "print(\"Best Model based on test R score:\")\n",
    "print(best_model_r2)\n",
    "\n",
    "# Select the best model based on Cross-Validation Mean R score\n",
    "best_models_cv_r2 = results_df.sort_values(by='CV_Mean_R2', ascending=False)\n",
    "best_model_cv_r2 = best_models_cv_r2.iloc[0]\n",
    "print(\"\\nBest Model based on Cross-Validation Mean R score:\")\n",
    "print(best_model_cv_r2)\n",
    "\n",
    "# Optionally, display the top 5 models based on each criterion\n",
    "top_5_models_r2 = best_models_r2.head(5)\n",
    "print(\"\\nTop 5 Models based on test R score:\")\n",
    "print(top_5_models_r2)\n",
    "\n",
    "top_5_models_cv_r2 = best_models_cv_r2.head(5)\n",
    "print(\"\\nTop 5 Models based on Cross-Validation Mean R score:\")\n",
    "print(top_5_models_cv_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>CV_Mean_R2</th>\n",
       "      <th>CV_Std_R2</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>STACKING_LINEAR_WITH_SVR_KNN</td>\n",
       "      <td>0.879771</td>\n",
       "      <td>0.100280</td>\n",
       "      <td>0.265883</td>\n",
       "      <td>0.124153</td>\n",
       "      <td>0.862197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>STACKING_ELASTICNET_WITH_SVR_GB</td>\n",
       "      <td>0.845409</td>\n",
       "      <td>0.089937</td>\n",
       "      <td>0.316762</td>\n",
       "      <td>0.303447</td>\n",
       "      <td>0.663190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>STACKING_ELASTICNET_WITH_SVR_GB_KNN</td>\n",
       "      <td>0.845409</td>\n",
       "      <td>0.089937</td>\n",
       "      <td>0.316762</td>\n",
       "      <td>0.303447</td>\n",
       "      <td>0.663190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>STACKING_ELASTICNET_WITH_SVR_RF_GB_KNN</td>\n",
       "      <td>0.845243</td>\n",
       "      <td>0.089816</td>\n",
       "      <td>0.319550</td>\n",
       "      <td>0.317048</td>\n",
       "      <td>0.648094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>STACKING_ELASTICNET_WITH_SVR_RF_GB</td>\n",
       "      <td>0.845243</td>\n",
       "      <td>0.089816</td>\n",
       "      <td>0.319550</td>\n",
       "      <td>0.317048</td>\n",
       "      <td>0.648094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Unnamed: 0  CV_Mean_R2  CV_Std_R2       MAE  \\\n",
       "14             STACKING_LINEAR_WITH_SVR_KNN    0.879771   0.100280  0.265883   \n",
       "202         STACKING_ELASTICNET_WITH_SVR_GB    0.845409   0.089937  0.316762   \n",
       "225     STACKING_ELASTICNET_WITH_SVR_GB_KNN    0.845409   0.089937  0.316762   \n",
       "243  STACKING_ELASTICNET_WITH_SVR_RF_GB_KNN    0.845243   0.089816  0.319550   \n",
       "223      STACKING_ELASTICNET_WITH_SVR_RF_GB    0.845243   0.089816  0.319550   \n",
       "\n",
       "          MSE        R2  \n",
       "14   0.124153  0.862197  \n",
       "202  0.303447  0.663190  \n",
       "225  0.303447  0.663190  \n",
       "243  0.317048  0.648094  \n",
       "223  0.317048  0.648094  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_5_models_cv_r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>CV_Mean_R2</th>\n",
       "      <th>CV_Std_R2</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>STACKING_LINEAR_WITH_SVR_RF</td>\n",
       "      <td>0.794076</td>\n",
       "      <td>0.183497</td>\n",
       "      <td>0.182704</td>\n",
       "      <td>0.073791</td>\n",
       "      <td>0.918096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>STACKING_LINEAR_WITH_RIDGE_SVR_BAYESIAN_RF</td>\n",
       "      <td>0.639876</td>\n",
       "      <td>0.283577</td>\n",
       "      <td>0.198073</td>\n",
       "      <td>0.078027</td>\n",
       "      <td>0.913394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>STACKING_LINEAR_WITH_RIDGE_SVR_BAYESIAN_RF_GB</td>\n",
       "      <td>0.404885</td>\n",
       "      <td>0.680023</td>\n",
       "      <td>0.192374</td>\n",
       "      <td>0.087071</td>\n",
       "      <td>0.903356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>STACKING_LINEAR_WITH_SVR_BAYESIAN_RF</td>\n",
       "      <td>0.728839</td>\n",
       "      <td>0.282440</td>\n",
       "      <td>0.213909</td>\n",
       "      <td>0.089995</td>\n",
       "      <td>0.900110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>STACKING_LINEAR_WITH_SVR_RF_GB</td>\n",
       "      <td>0.745078</td>\n",
       "      <td>0.201218</td>\n",
       "      <td>0.181319</td>\n",
       "      <td>0.091967</td>\n",
       "      <td>0.897922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Unnamed: 0  CV_Mean_R2  CV_Std_R2  \\\n",
       "12                    STACKING_LINEAR_WITH_SVR_RF    0.794076   0.183497   \n",
       "41     STACKING_LINEAR_WITH_RIDGE_SVR_BAYESIAN_RF    0.639876   0.283577   \n",
       "56  STACKING_LINEAR_WITH_RIDGE_SVR_BAYESIAN_RF_GB    0.404885   0.680023   \n",
       "31           STACKING_LINEAR_WITH_SVR_BAYESIAN_RF    0.728839   0.282440   \n",
       "34                 STACKING_LINEAR_WITH_SVR_RF_GB    0.745078   0.201218   \n",
       "\n",
       "         MAE       MSE        R2  \n",
       "12  0.182704  0.073791  0.918096  \n",
       "41  0.198073  0.078027  0.913394  \n",
       "56  0.192374  0.087071  0.903356  \n",
       "31  0.213909  0.089995  0.900110  \n",
       "34  0.181319  0.091967  0.897922  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_5_models_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing the best model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of the New Stacking Model:\n",
    "Cross-validation MAE: 0.2275\n",
    "Cross-validation MSE: 0.0898\n",
    "Cross-validation R2: 0.8369\n",
    "Test MAE: 0.2103\n",
    "Test MSE: 0.0951\n",
    "Test R2: 0.8945\n",
    "\n",
    "\n",
    "## Previous Best Models from the Tables:\n",
    "\n",
    "### Top Models Based on Cross-Validation Mean R Score:\n",
    "\n",
    "Stacking_linear_with_svr_knn\n",
    "CV_Mean_R2: 0.8798\n",
    "CV_Std_R2: 0.1003\n",
    "MAE: 0.2659\n",
    "MSE: 0.1242\n",
    "R2: 0.8622\n",
    "\n",
    "### Top Models Based on Test R Score:\n",
    "\n",
    "Stacking_linear_with_svr_rf\n",
    "CV_Mean_R2: 0.7941\n",
    "CV_Std_R2: 0.1835\n",
    "MAE: 0.1827\n",
    "MSE: 0.0738\n",
    "R2: 0.9181\n",
    "\n",
    "## Comparison:\n",
    "### Cross-Validation Metrics:\n",
    "\n",
    "New Stacking Model: CV_R2: 0.8369, CV_MAE: 0.2275, CV_MSE: 0.0898\n",
    "Stacking_linear_with_svr_knn: CV_R2: 0.8798, CV_MAE: 0.2659, CV_MSE: 0.1242\n",
    "Stacking_linear_with_svr_rf: CV_R2: 0.7941, CV_MAE: 0.1827, CV_MSE: 0.0738\n",
    "\n",
    "\n",
    "### Test Set Metrics:\n",
    "\n",
    "New Stacking Model: R2: 0.8945, MAE: 0.2103, MSE: 0.0951\n",
    "Stacking_linear_with_svr_knn: R2: 0.8622, MAE: 0.2659, MSE: 0.1242\n",
    "Stacking_linear_with_svr_rf: R2: 0.9181, MAE: 0.1827, MSE: 0.0738\n",
    "\n",
    "## Conclusion:\n",
    "\n",
    "## Best Model Based on Test Set R Score:\n",
    "\n",
    "### Stacking_linear_with_svr_rf: Test R: 0.9181, Test MAE: 0.1827, Test MSE: 0.0738\n",
    "\n",
    "### Best Model Based on Cross-Validation Mean R Score:\n",
    "\n",
    "### Stacking_linear_with_svr_knn: CV_R2: 0.8798, CV_MAE: 0.2659, CV_MSE: 0.1242\n",
    "\n",
    "New Stacking Model:\n",
    "\n",
    "\n",
    "Test R: 0.8945 (second highest)\n",
    "CV_R2: 0.8369 (second highest among provided models)\n",
    "\n",
    "The new stacking model performs very well in terms of test set performance and has solid cross-validation results.\n",
    "\n",
    "### Given the performance metrics, Stacking_linear_with_svr_rf remains the best model overall due to its highest R score on the test set. \n",
    "\n",
    "However, the new stacking model with base models ridge, svr, and bayesian, and a LinearRegression final estimator also performs exceptionally well and could be a very strong alternative.\n",
    "\n",
    "## Final Decision:\n",
    "Primary Choice: Stacking_linear_with_svr_rf\n",
    "\n",
    "Strong Alternative: The new stacking model (ridge, svr, bayesian with LinearRegression as final estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Results:\n",
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, CV_Mean_R2, CV_Std_R2, MAE, MSE, R2]\n",
      "Index: []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Stacking_new_model_ridge_svr_bayesian.pkl']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'Stacking_linear_with_svr_rf' is the best model based on the analysis\n",
    "best_model_name = 'Stacking_linear_with_svr_rf'\n",
    "\n",
    "# Load the best model\n",
    "best_model = joblib.load(f\"{best_model_name}.pkl\")\n",
    "\n",
    "# Save the best model with a new name for clarity\n",
    "joblib.dump(best_model, 'best_stacking_model.pkl')\n",
    "\n",
    "# Document the final results\n",
    "best_model_results = results_df.loc[results_df.index == best_model_name]\n",
    "print(\"Best Model Results:\")\n",
    "print(best_model_results)\n",
    "\n",
    "# Save the best model results to a CSV file\n",
    "best_model_results.to_csv('best_model_results.csv', index=False)\n",
    "\n",
    "# Optionally, save the strong alternative model\n",
    "alternative_model_name = 'Stacking_new_model_ridge_svr_bayesian'\n",
    "joblib.dump(stacking_model, f\"{alternative_model_name}.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
